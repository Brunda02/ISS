
lecture 1:

In this lecture we are trying to find the solutions of the tractable problems

We will see various algorithms to find the n^{th} fibonacci number F_{n} from the naive to the more optimized ones

1. Defintion of fibonacci:

  The Fibonacci numbers are a sequence F_{n} of integers in which every number after the first two, 0 and 1, is the sum of the two preceding numbers: 0,1,1,2,3,5,8,13,21,... 

                F_{n-1}+F_{n-2} if n>1
      F_{n} =          1        if n=1
                       0        if n=0

    In this problem the input of the program is n and the output is F_{n}

2. 1: Recursive approach for solving
    
    fib(n):
    if n<=1 : return n
    else return F_{n-1}+F_{n-2}

    This algorithm is straight forward and we can say that it is correct according to the definition. Now if we talk about the time complexity

    T(n)<=2  for n<=1 because the values of F_{0} and F_{1} are pre-defined by definition.
                                 
    if n>1 then we can write that   T(n)=T(n-1)+T(n-2)+3   where 3 is the additional workdonne outside the recursion.
    If we see the above equation and compare it with the definition of the fibonacci there is an extra constant 3 so we can definitely say that T(n) >= F_{n}
    Also we know the fact that n^{th} fibonacci number has approximate value of 2^{0.694n} and number of digits would be ome constant times n (say c*n, number of digits in the number is equal to mantissa of log_10^{2^{0.694n}}= 0.694*log_10{2}*n = c*n)
     
    Hence, T(n) >= 2^{0.694n}  so the Time complexity grows exponentially......

    We can do better than this algorithm because in the recurrence relation F_{n}=F_{n-1}+F_{n-2} we are calculating same values multiple times, Example: F_{n}=F_{n-1}+F_{n-2} next F_{n-1}=F_{n-2}+F_{n-3} and F_{n-2}=F_{n-3}+F_{n-4} that is we are calculating F_{n-2},F_{n-3} two times which is clearly a waste of time. So, next approach is to avoid this

3. 2: Iterative approch for solving

   fib2(n):
     array f[n+1];
     f(0)=0, f[1]=1
     for i in range(2,n+1):
       f[n]=f[n-1]+f[n-2];
     return f[n];

  This approach uses memoization that is storing the already computed values in a array and using them just by refering to index
  It appears to be an algorithm with linear time complexity but actually it is not. We found that the number of digits in n^{th} fibonacci number is c*n where c is a constant.

        So, each addition (F_{n-1}+F_{n-2}) takes time of O(n) so the total time comlexity = O(n^{2})

  The next step is to think whether can we achieve better time complexity than this.

4. 3: Using matrix matrix multiplications

   There is a interesting relationship between the fibonacci numbers using matrices.

     F_{n}     =    0 1 F_{0}
     F_{n+1}        1 1 F_{1}

   In this algorithm time complexity depends on the time taken for for mutiplying two n-bit integers

5. Algorithm for multiplying two n-bit integers(Karatsuba Algorithm)
   
   The naive approach for multiplying is taking one-bit of an integer and multiplying it with all n bits of another integer(0*0=0*1=1*0=0 and 1*1=1) so a total of order of n^{2} single digit multiplications, additions and shift operations will be performed.

   Therefore time complexity of this naive approach = O(n^{2})
   Now, we have to check whether we can do better than this or not.

   If we see the multiplication of two complex numbers a+ib, c+id
     (a+ib)*(c+id) = (ac-bd)+i(bc+ad)
   That is we have to perform four multiplications. But we can do better than this by performing only 3 multiplications
   To find (bc+ad) we can compute ac, bd, (a+b)(c+d) and do (a+b)(c+d)-ac-bd. Here we are performing only 3 multiplications
   So, if we want to do the multiplications of two n-bit integers, we can do this way
   1. Add two n/2 integers
   2. Multiply three n/2 integers
   3. Add, subtract, and shift n/2 digit integers to obtain result

   x = 2^{\frac{n}{2}}x_{1} + x_{0} 
   ![first eqn](https://latex.codecogs.com/gif.latex?x%20%3D%202%5E%7B%5Cfrac%7Bn%7D%7B2%7D%7Dx_%7B1%7D%20&plus;%20x_%7B0%7D)
   y = 2^{\frac{n}{2}}y_{1} + y_{0}
   x.y = (2^{\frac{n}{2}}x_{1} + x_{0}).(2^{\frac{n}{2}}y_{1} + y_{0})
       = 2^{n}.x_{1}.y_{1} + 2^{\frac{n}{2}}.((x_{0}+x_{1}).(y_{0}+y_{1})-(x_{1}.y_{1}+x_{0}.y_{0}))+x_{0}.y_{0}

   So the equation for time complexity can be written as

       T(n) = T(\frac{n}{2}) + T(\frac{n}{2}) + T(\frac{n}{2} + 1)  + \theta (n)
       Using master theorem 3rd case we get O(n^{log_{2}^{3}}) Therefore it is equal to O(n^{1.585})